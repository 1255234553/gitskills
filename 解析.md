# 解析

## 1. xpath

xpath使用：

提前安装xpath插件

ctrl + shift +x

​	(1) 安装lxml库

​	(2) 导入lxml.etree

​		`from lxml import etree`

​    (3) etree.parse()	解析本地文件

​		`html_tree = etree.parse('XX.html')`

​    (4) etree.HTML()	服务器响应文件

​		`html_tree = etree.HTML(response.read().decode('utf-8'))`

​	(5) `html_tree.xpath(xpath路径)`



xpath基本语法：

1. 路径查询

   // : 查找所有子孙节点，不考虑层级关系

   /  :找直接子节点

2. 谓词查询

   //div[@id]

   //div[@id="maincontent"]

3. 属性查询

   //@class

4. 模糊查询

   //div[contains(@id,"he")]

   //div[start-with(@id,"he")]

5. 内容查询

   //div/h1/text()

6. 逻辑运算

   //div[@id="head" and @class="s_down"]

   //title | //price



```python
from lxml import etree

# xpath解析
# （1）本地文件                                                   etree.parse()
# （2）服务器响应的数据   response.read().decode('utf-8') *****    etree.HTML()

# xpath解析本地文件
tree = etree.parse('解析.html')


# tree.xpath('xpath路径')

# 查找ul下面的li
# li_list = tree.xpath('//body/ul/li')    # //body//li



# 查找所有有id的属性的li标签
# text()获取标签中的内容
# li_list = tree.xpath('//ul/li[@id]/text()')

# 找到id为l1的li标签  注意引号的问题
# li_list = tree.xpath('//ul/li[@id="l1"]/text()')


# 查找到id为l1的li标签的class的属性值
# li = tree.xpath('//ul/li[@id="l1"]/@class')

# 查询id中包含l的li标签
# li_list = tree.xpath('//ul/li[contains(@id,"l")]/text()')

# 查询id的值以l开头的li标签
# li_list = tree.xpath('//ul/li[starts-with(@id,"c")]/text()')


# 查询id为l1和class为c1的
# li_list = tree.xpath('//ul/li[@id="l1" and @class="c1"]/text()')

#
li_list = tree.xpath('//ul/li[@id="l1" ]/text() | //ul/li[@id="l2"]/text()')

# 判断列表的长度
print(li_list)
print(len(li_list))
```

```python
# （1） 获取网页源码
# （2） 解析    解析的服务器响应文件  etree.HTML
# （3） 打印

import urllib.request

url = 'https://www.baidu.com/'

headers = {
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'
}

# 请求对象的定制
request = urllib.request.Request(url=url,headers=headers)

# 模拟浏览器访问服务器
response = urllib.request.urlopen(request)

# 获取网页源码
content = response.read().decode('utf-8')

# 解析网页源码 来获取我们想要的数据
from lxml import etree

# 解析服务器响应的文件
tree = etree.HTML(content)

# 获取想要的数据   xpath的返回值是一个列表类型的数据
result = tree.xpath('//input[@id="su"]/@value')[0]

print(result)
```

```python
# 需求：下载前十页的图片
# https://sc.chinaz.com/tupian/qinglvtupian_1.html
# https://sc.chinaz.com/tupian/qinglvtupian_2.html

import urllib.request
from lxml import etree
def create_request(page):
    if(page == 1):
        url = 'https://sc.chinaz.com/tupian/qinglvtupian.html'
    else:
        url = 'https://sc.chinaz.com/tupian/qinglvtupian_' + str(page) + '.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'
    }

    request = urllib.request.Request(url=url,headers=headers)
    return request

def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content
def down_load(content):
#   下载图片
    # urllib.request.urlretrieve('图片地址','文件名字')
    tree = etree.HTML(content)

    name_list = tree.xpath('//div[@id="container"]//a/img/@alt')

    # 一般涉及到图片的网站都会进行懒加载
    src_list = tree.xpath('//div[@id="container"]//a/img/@src2')

    # print(len(name_list),len(src_list))
    # for name in name_list:
    #     print(name)

    for i in range(len(name_list)):
        name = name_list[i]
        src = src_list[i]
        url = 'https:' + src

        urllib.request.urlretrieve(url=url,filename='./loveImg/' + name + '.jpg')






if __name__ == '__main__':
    start_page = int(input('请输入起始页码'))
    end_page = int(input('请输入结束页码'))

    for page in range(start_page,end_page+1):

        # （1）请求对象的定制
        request = create_request(page)
        # （2）获取网页源码
        content = get_content(request)
        # （3）下载
        down_load(content)
```



## 2.JsonPath

jsonpath安装：`pip install jsonpath`

使用：`obj = json.load(open('json文件', 'r', enconding='utf-8'))`

​			`ret = jsonpath.jsonpath(obj,'jsonpath语法')`

```python
import json
import jsonpath

obj = json.load(open('a','r',encoding='utf-8'))

# 书店所有书的作者
# author_list = jsonpath.jsonpath(obj, '$.store.book[*].author')
# print(author_list)

# 所有的作者
# author_list = jsonpath.jsonpath(obj,'$..author')
# print(author_list)

# store下面所有的元素
# tag_list = jsonpath.jsonpath(obj,'$.store.*')
# print(tag_list)


# store里面所有东西的price
# price_list = jsonpath.jsonpath(obj,'$.store..price')
# print(price_list)

# 第三个书
# book = jsonpath.jsonpath(obj,'$..book[2]')
# print(book)

# 最后一本书
# book =jsonpath.jsonpath(obj,'$..book[(@.length-1)]')
# print(book)

# 前两本书
# book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
# book_list = jsonpath.jsonpath(obj,'$..book[:2]')
# print(book_list)

# 条件过滤需要在（）前面添加个？
# 过滤出所有包含isbn的书
# book_list = jsonpath.jsonpath(obj,'$..book[?(@.isbn)]')
# print(book_list)

# 哪本书超过了10块钱
# book_list = jsonpath.jsonpath(obj,'$..book[?(@.price>10)]')
# print(book_list)
```

```python
import urllib.request

url = 'https://dianying.taobao.com/cityAction.json?activityId&_ksTS=1634734268550_137&jsoncallback=jsonp138&action=cityAction&n_s=new&event_submit_doGetAllRegion=true'

headers = {
    # ':authority':' dianying.taobao.com',
    # ':method':' GET',
    # ':path':' /cityAction.json?activityId&_ksTS=1634734268550_137&jsoncallback=jsonp138&action=cityAction&n_s=new&event_submit_doGetAllRegion=true',
    # ':scheme':' https',
    'accept':' text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
    # 'accept-encoding':' gzip, deflate, br',
    'accept-language':' zh-CN,zh;q=0.9',
    'cookie':' miid=3483985191112321420; t=bc6be797e4a2cd1bfc3a8a7dc14a0429; cookie2=199ebe2369d4494701c29f66d5ea6f6f; v=0; _tb_token_=e691e03b3b13a; cna=DhrGGWq6v0ACATz3KZl2GcdW; xlly_s=1; tb_city=110100; tb_cityName="sbG+qQ=="; tfstk=cghFBPaPcBdeUDPWHWNyNdJMxZFdZemK2ia0KvjVjQhp_72hiKj8Sn-7hJQNqJf..; l=eBON5ESrgy9p3G9aBO5CFurza77TrIRbzsPzaNbMiInca6Z19F6JNNCLko3XWdtjgt5xEetzoziGYRF28fUU-VsWHpfuKtyuJHv6Re1..; isg=BISEcX2XuQcKDQ3UsdlBSZavVQJ2nagHZMc0xp4l3M8SySaTxqzFllTnCWERUeBf',
    'referer': 'https://dianying.taobao.com/',
    'sec-ch-ua':' "Chromium";v="94", "Google Chrome";v="94", ";Not A Brand";v="99"',
    'sec-ch-ua-mobile':' ?0',
    'sec-ch-ua-platform':' "Windows"',
    'sec-fetch-dest':' empty',
    'sec-fetch-mode':' cors',
    'sec-fetch-site':' same-origin',
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36',
    'x-requested-with':' XMLHttpRequest',
}

request = urllib.request.Request(url=url,headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

# split切割
content = content.split('(')[1].split(')')[0]

with open('jsonpath解析淘票票.json','w',encoding='utf-8')as fp:
    fp.write(content)

import json
import jsonpath

obj = json.load(open('jsonpath解析淘票票.json','r',encoding='utf-8'))

city_list = jsonpath.jsonpath(obj,'$..regionName')

print(city_list)
```



## 3. BeautifulSoup

**基本简介**

```
1. BeautifulSoup简称：bs4
2. BeautifulSoup和lxml一样，是一个html解析器，主要功能也是解析和提取数据
3. 缺点：效率没有lxml的效率高
   优点：接口设计人性化，使用方便
```

**安装及创建**

```
1. 安装
	pip install bs4
2. 导入
	from bs4 import BeautifulSoup
3. 创建对象
	服务器响应的文件生成对象
		soup = BeautifulSoup(response.read().decode(),'lxml')
	本地文件生成对象
		soup = BeautifulSoup(open('1.html'),'lxml')
		注意：默认打开文件的编码格式gbk所以需要指定打开编码格式
```

```python
from bs4 import BeautifulSoup

# 通过解析本地文件 来将bs4的基础语法进行讲解
# 默认打开的文件的编码格式是gbk，所有在打开文件的时候需要指定编码
soup = BeautifulSoup(open('bs4的基本使用.html',encoding='utf-8'),'lxml')

# 根据标签页面查找节点
# 找到的是第一个符合条件的数据
# print(soup.a)
# # 获取标签的属性和属性值
# print(soup.a.attrs)

# bs4的一些函数
# (1)find
# 返回的是第一个符合条件的数据
# print(soup.find('a'))
# 根据title的值找到对应的标签对象
# print(soup.find('a',title="a2"))

# 根据class的值来找到对应的标签对象   注意的是class需要添加下划线
# print(soup.find('a',class_="a1"))





# (2)find_all   返回的是一个列表    并且返回了所有的a标签
# print(soup.find_all('a'))

# 如果想获取的是多个标签的数据 那么需要在find_all的参数中添加的是列表的数据
# print(soup.find_all(['a','span']))

# limit作用是查找前几个数据
# print(soup.find_all('li',limit=2))



# (3)select（推荐）
# select方法返回的是一个列表并且会返回多个数据
# print(soup.select('a'))

# 可以通过.代表class  我们把这种操作叫做类选择器
# print(soup.select('.a1'))

# print(soup.select('#l1'))


# 属性选择器：通过属性来寻找对应的标签
# 查找到li标签中有id的标签
# print(soup.select('li[id]'))

# 查找到li标签中id为l2的标签
# print(soup.select('li[id="l2"]'))




# 层级选择器
# 后代选择器
# 找到的是div下面的li
# print(soup.select('div li'))

# 子代选择器
# 某标签的第一级子标签
# 注意：很多的计算机编程语言中，如果不加空格不会输出内容，但是在bs4中不会报错会显示内容
# print(soup.select('div > ul > li'))

# 找到a标签和li标签的所有对象
# print(soup.select('a,li'))



# 节点信息
# 获取节点内容
# obj = soup.select('#d1')[0]
# # 如果标签对象中只有内容那么string和get_text()都可以使用
# # 我们一般情况下   推荐使用get_text()
# print(obj.string)
# print(obj.get_text())




# 节点的属性
# obj = soup.select('#p1')[0]
# name是标签的名字
# print(obj.name)
# 将属性值作为一个字典返回
# print(obj.attrs)

# 获取节点的属性
# obj = soup.select('#p1')[0]
#
# print(obj.attrs.get('class'))
# print(obj.get('class'))
# print(obj['class'])
```

```python
import urllib.request

url = 'https://www.starbucks.com.cn/menu/'

response = urllib.request.urlopen(url)

content = response.read().decode('utf-8')

from bs4 import BeautifulSoup

soup = BeautifulSoup(content,'lxml')

# //ul[@class="grid padded-3 product"]//strong/text()
name_list = soup.select('ul[class="grid padded-3 product"] strong')

for name in name_list:
    # print(name.string)
    print(name.get_text())
```

